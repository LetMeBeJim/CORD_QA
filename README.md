# CORD_QA

This project aims to evaluate the feasibility of creating a Transformer-based Question-Answering (Q&A) model tailored to the COVID-19 Open Research Dataset (CORD-19). The team performed exploratory data analysis including TF-IDF and n-grams analysis, used BERTopic to identify key topics, generated extractive summaries using LexRank, and produced abstractive summaries using the BART model. Subsequently, a Transformer-based extractive Q&A model was trained using a Q&A dataset generated by applying a zero-shot Large-Language Model (LLM) on entities identified from the extractive summaries via spaCyâ€™s part-of-speech (POS) tagging. Additionally, the project involved extensive experimentation with various models and hyperparameters to optimize Q&A model training. This project demonstrates innovative methodologies and promising results on repurposing a Transformer model on a new knowledge base through performing zero-shot question generation using a LLM
